{"cells":[{"cell_type":"code","execution_count":1,"id":"01b1ac6d-df1b-4bef-acda-216025ee4048","metadata":{},"outputs":[],"source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":5,"id":"6e9ea38a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\n"]}],"source":["# import os\n","\n","# # Define the target directory\n","# target_directory = r\"C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-search-indexing\"  # change your directory here\n","\n","# # Check if the directory exists\n","# if os.path.exists(target_directory):\n","#     # Change the current working directory\n","#     os.chdir(target_directory)\n","#     print(f\"Directory changed to {os.getcwd()}\")\n","# else:\n","#     print(f\"Directory {target_directory} does not exist.\")"]},{"cell_type":"code","execution_count":9,"id":"dc583787","metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - lighter</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://LAPTOP-IPPB4T44:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.4.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[1]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Data Cloud: Spark Lighter Connection 2024-04-12 19:12:19.967836</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x2c1feaea170>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":null,"id":"4e371986","metadata":{},"outputs":[],"source":["#%pip install -r builtin/requirements.txt"]},{"cell_type":"code","execution_count":2,"id":"14b9a578","metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from dotenv import load_dotenv\n","import os\n","\n","# specify the path to your .env file\n","dotenv_path = 'C:\\\\Users\\\\pablosal\\\\Desktop\\\\gbbai-azure-ai-fabric-aoai\\\\.env'\n","\n","# load the variables\n","load_dotenv(dotenv_path)"]},{"cell_type":"code","execution_count":3,"id":"0febd7f7","metadata":{},"outputs":[],"source":["from synapse.ml.core.platform import find_secret\n","\n","service_url = os.getenv('AZURE_AOAI_API_ENDPOINT')\n","deployment_name = os.getenv('AZURE_AOAI_EMBEDDING_DEPLOYMENT_ID')\n","key = os.getenv('AZURE_AOAI_API_KEY')"]},{"cell_type":"code","execution_count":4,"id":"6d60f2b7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3.4.1\n"]}],"source":["import pyspark\n","print(pyspark.__version__)"]},{"cell_type":"code","execution_count":5,"id":"1ef32636","metadata":{},"outputs":[],"source":["from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","from synapse.ml.services import OpenAIEmbedding\n","from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":17,"id":"cc4a197d","metadata":{},"outputs":[{"data":{"text/plain":["[Row(id=529, Time=1351209600, ProductId='B0007A0AQW', UserId='A1HHNY1GVD3UIS', Score=5, Summary='my dogs love the peanut butter!', Text=\"First off, read the ingredients, no crazy words I can't pronounce, which means it's all natural! I got the peanut butter treats for my two children: husky/shepherd and a lab/shepherd mix, and they can't get enough of them. Plus, on Amazon, this is such a great value for 16oz. Beats petsmart for sure! I'll be looking for more Zuke's products now that I've discovered them. Thanks Zuke's!\", combined=\"Title: my dogs love the peanut butter!; Content: First off, read the ingredients, no crazy words I can't pronounce, which means it's all natural! I got the peanut butter treats for my two children: husky/shepherd and a lab/shepherd mix, and they can't get enough of them. Plus, on Amazon, this is such a great value for 16oz. Beats petsmart for sure! I'll be looking for more Zuke's products now that I've discovered them. Thanks Zuke's!\"),\n"," Row(id=132, Time=1351123200, ProductId='B0012KB3ZI', UserId='AIFGUCOUOOFJ3', Score=5, Summary='Great for my older cat', Text='My older cat just stopped eating the dry kibble food a few months ago. She used to get Fancy Feast as a treat only, but now it is all she will eat. A lot of canned cat food smells repulsive! Fancy Feast actually looks & smells decent. The cat knows which cabinet I keep her food in & she sprints to the kitchen when she hears it open.', combined='Title: Great for my older cat; Content: My older cat just stopped eating the dry kibble food a few months ago. She used to get Fancy Feast as a treat only, but now it is all she will eat. A lot of canned cat food smells repulsive! Fancy Feast actually looks & smells decent. The cat knows which cabinet I keep her food in & she sprints to the kitchen when she hears it open.'),\n"," Row(id=764, Time=1351209600, ProductId='B001HTIUDC', UserId='A27RDA09KDYVGY', Score=5, Summary='Delicious', Text='This product is a great alternative to peanut butter, or any butter. I like it just as much as I ever liked peanut butter. It has a similar nutty flavor, with a great contrast of salty and sweet. The shipping was fast, and its a delicious and healthy snack you can eat with just about anything!', combined='Title: Delicious; Content: This product is a great alternative to peanut butter, or any butter. I like it just as much as I ever liked peanut butter. It has a similar nutty flavor, with a great contrast of salty and sweet. The shipping was fast, and its a delicious and healthy snack you can eat with just about anything!'),\n"," Row(id=833, Time=1351209600, ProductId='B0054ES1NK', UserId='A25QCYSED869Q3', Score=5, Summary='If you like mint candy......', Text=\"This is the best hard mint candy in the world.  Expensive, though.  I can't find it in stores.  Anyone know where I can get it off-line?\", combined=\"Title: If you like mint candy......; Content: This is the best hard mint candy in the world.  Expensive, though.  I can't find it in stores.  Anyone know where I can get it off-line?\"),\n"," Row(id=495, Time=1351209600, ProductId='B0012VSXIM', UserId='A2TOUMUWSN7FJY', Score=5, Summary='Good & Plenty Licorice Candy', Text='If you like licorice you will love this candy.  I can remember eating this candy from a box at the local movie theatre when I was a kid and it is still just as good.', combined='Title: Good & Plenty Licorice Candy; Content: If you like licorice you will love this candy.  I can remember eating this candy from a box at the local movie theatre when I was a kid and it is still just as good.')]"]},"metadata":{},"output_type":"display_data"}],"source":["import pyspark.sql.functions as F\n","\n","df = (\n","    spark.read.options(inferSchema=\"True\", delimiter=\",\", header=True)\n","    .csv(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/fine_food_reviews_1k.csv\")\n","    .repartition(5)\n",")\n","\n","df = df.withColumn(\n","    \"combined\",\n","    F.format_string(\"Title: %s; Content: %s\", F.trim(df.Summary), F.trim(df.Text)),\n",")\n","\n","display(df.head(5))\n","df.write.mode(\"overwrite\").format(\"parquet\").save(\"Files/ \" + \"fine_food_reviews_1k.parquet\")"]},{"cell_type":"code","execution_count":6,"id":"3045f06a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["df is a Spark DataFrame\n"]}],"source":["df = spark.read.parquet(\"Files/ fine_food_reviews_1k.parquet\") \n","from pyspark.sql import DataFrame\n","\n","if isinstance(df, DataFrame):\n","    print(\"df is a Spark DataFrame\")\n","else:\n","    print(\"df is not a Spark DataFrame\")"]},{"cell_type":"code","execution_count":null,"id":"c43110c4","metadata":{},"outputs":[],"source":["# importing libraries\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","from pyspark.sql import SparkSession\n","from pyspark.sql import Row\n","\n","import pandas as pd\n","\n","import openai\n","import os\n","\n","import requests\n","import json\n","\n","import aiohttp\n","import asyncio\n","import nest_asyncio\n","\n","import ssl\n","\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from threading import Lock"]},{"cell_type":"code","execution_count":null,"id":"21946f09","metadata":{},"outputs":[],"source":["# Apply the necessary patch for asyncio to utilise exisiting event loop, if running in Jupyter\n","nest_asyncio.apply()\n","\n","# SSL context for self-signed certificate\n","## Please note that this is an optional parameter if you're not using a self-hosted endpoint\n","ssl_context = ssl.create_default_context(cafile=certificate_path)\n","\n","# Function to make asynchronous call to Azure OpenAI API\n","async def async_query_gpt4(session, api_key, prompt, ssl_context=ssl_context):\n","    url = api_url  # Adjust as needed\n","    headers = {\n","        \"Content-Type\": \"application/json\",\n","        \"api-key\": f\"{api_key}\"\n","    }  \n","    data = {\n","        \"model\": \"gpt-4\",  # Adjust the model as needed\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ]\n","    }\n","\n","    async with session.post(url, json=data, headers=headers, ssl=ssl_context) as response:\n","        return await response.json()\n","\n","# Function to perform asynchronous tasks with Semaphore for rate limiting\n","async def async_task(api_key, prompts, ssl_context=ssl_context, max_concurrent_requests=MAX_THREADS):\n","    async with aiohttp.ClientSession() as session:\n","        semaphore = asyncio.Semaphore(max_concurrent_requests)\n","        tasks = []\n","        responses = []\n","\n","        async def fetch_and_append(prompt):\n","            async with semaphore:\n","                response = await async_query_gpt4(session, api_key, prompt, ssl_context)\n","                responses.append(response)\n","\n","        for prompt in prompts:\n","            task = asyncio.create_task(fetch_and_append(prompt))\n","            tasks.append(task)\n","            # You can add logic here to adjust batch sizes adaptively\n","\n","        await asyncio.gather(*tasks)\n","        return responses\n","        \n","def process_prompts(DF, prompt_col, response_col='response', max_concurrent_requests=MAX_THREADS):\n","\n","    # create a prompt index and extract prompts into a list\n","    DF = DF.withColumn(\"prompt_idx\", F.monotonically_increasing_id())\n","\n","    df = DF[['prompt_idx', prompt_col]].toPandas()\n","    prompts_idx = df['prompt_idx'].to_list()\n","    prompts = df[prompt_col].to_list()\n","    \n","    # Run async tasks and get results\n","    loop = asyncio.get_event_loop()\n","    prompt_responses = loop.run_until_complete(async_task(api_key, prompts, ssl_context, max_concurrent_requests))\n","\n","    # extracting content from the API responses\n","    responses = [response['choices'][0]['message']['content'] for response in prompt_responses]\n","\n","    # converting results into a \n","    results = list(zip(prompts_idx, responses))\n","    results_DF = spark.createDataFrame(results, [\"prompt_idx\", response_col])\n","\n","    # Join back with original DataFrame using the index\n","    DF = DF.join(results_DF, on=['prompt_idx']).drop('prompt_idx')\n","\n","    return DF\n","    \n","# read the dataframe with prompts\n","responseDF = spark.sql(sql_statement)\n","\n","# Using pandas converted dataframe to make asynchronous calls\n","responseDF = process_prompts(DF=responseDF, prompt_col='prompt', response_col=\"response\")"]},{"cell_type":"code","execution_count":7,"id":"a9a33be9","metadata":{},"outputs":[{"ename":"TypeError","evalue":"'JavaPackage' object is not callable","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msynapse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbedding\n\u001b[0;32m      3\u001b[0m embedding \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mOpenAIEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39msetSubscriptionKey(key)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39msetDeploymentName(deployment_name)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39msetCustomServiceName(service_url)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39msetTextCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39msetErrorCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m completed_df \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39mtransform(df)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m     14\u001b[0m display(completed_df)\n","File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\fabric-synapse-runtime-1-2\\lib\\site-packages\\pyspark\\__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\fabric-synapse-runtime-1-2\\lib\\site-packages\\synapse\\ml\\services\\openai\\OpenAIEmbedding.py:101\u001b[0m, in \u001b[0;36mOpenAIEmbedding.__init__\u001b[1;34m(self, java_obj, AADToken, AADTokenCol, CustomAuthHeader, CustomAuthHeaderCol, apiVersion, apiVersionCol, concurrency, concurrentTimeout, deploymentName, deploymentNameCol, errorCol, handler, outputCol, subscriptionKey, subscriptionKeyCol, text, textCol, timeout, url, user, userCol)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28msuper\u001b[39m(OpenAIEmbedding, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.microsoft.azure.synapse.ml.services.openai.OpenAIEmbedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m java_obj\n","File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\fabric-synapse-runtime-1-2\\lib\\site-packages\\pyspark\\ml\\wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[1;34m(java_class, *args)\u001b[0m\n\u001b[0;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[0;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjava_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"]}],"source":["from synapse.ml.services.openai import OpenAIEmbedding\n","\n","embedding = (\n","    OpenAIEmbedding()\n","    .setSubscriptionKey(key)\n","    .setDeploymentName(deployment_name)\n","    .setCustomServiceName(service_url)\n","    .setTextCol(\"combined\")\n","    .setErrorCol(\"error\")\n","    .setOutputCol(\"embeddings\")\n",")\n","\n","completed_df = embedding.transform(df).cache()\n","display(completed_df)"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"729b9f89-fa73-49b3-8d2c-d9336156a8a4","default_lakehouse_name":"Gbbai_Lakehouse","default_lakehouse_workspace_id":"ad1c5fea-959e-4bc7-a5a7-15cc929c57d3","known_lakehouses":[{"id":"729b9f89-fa73-49b3-8d2c-d9336156a8a4"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"fabric-synapse-runtime-1-2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
